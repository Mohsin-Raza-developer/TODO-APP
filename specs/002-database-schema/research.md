# Research: Database Schema Technology Decisions

**Feature**: Database Schema for Multi-User Todo Application
**Date**: 2026-01-12
**Status**: Complete

## Overview

This document consolidates research findings for implementing the database schema using SQLModel, Alembic, and Neon PostgreSQL. All technical unknowns from the planning phase have been resolved with concrete decisions, rationales, and code examples.

---

## 1. SQLModel Model Definition Best Practices

### Decision: Use sa_column for Database-Specific Configuration

**Rationale**: SQLModel is a wrapper around SQLAlchemy + Pydantic. For database-specific configurations (like BIGINT primary keys, server defaults, indexes), we must use `sa_column` parameter in `Field()` to pass SQLAlchemy Column objects.

### BIGINT Primary Keys

```python
from sqlmodel import SQLModel, Field
from sqlalchemy import Column, BigInteger

class Task(SQLModel, table=True):
    """Task model with BIGINT primary key for ID Architect pattern."""
    __tablename__ = "tasks"

    id: int | None = Field(
        default=None,
        sa_column=Column(BigInteger, primary_key=True, autoincrement=True)
    )
    # BIGINT maps to Python int, supports ~9.2 quintillion values
```

**Alternatives Considered**:
- `id: int = Field(primary_key=True)` - Uses INTEGER (max 2.1 billion), rejected due to exhaustion risk
- UUID primary keys - Not human-readable, rejected for UX reasons (violates ID Architect pattern)

### Foreign Key References

```python
from sqlmodel import Field, Relationship

class Task(SQLModel, table=True):
    user_id: str = Field(
        foreign_key="users.id",  # References users table, id column
        nullable=False,          # Enforces NOT NULL constraint
        index=True               # Creates index automatically (SQLModel 0.0.14+)
    )

    # Optional: Define relationship for ORM navigation
    # user: "User" = Relationship(back_populates="tasks")
```

**Key Insight**: `index=True` in Field() creates B-tree index automatically in Alembic autogeneration. However, for explicit control, we'll create index manually in migration (see Alembic section).

### Timestamp Defaults

```python
from sqlalchemy import Column, TIMESTAMP, text

class Task(SQLModel, table=True):
    created_at: datetime = Field(
        sa_column=Column(
            TIMESTAMP,
            nullable=False,
            server_default=text("NOW()")  # Database-level default, not Python
        )
    )

    updated_at: datetime = Field(
        sa_column=Column(
            TIMESTAMP,
            nullable=False,
            server_default=text("NOW()")
        )
    )
```

**Rationale**: `server_default=text("NOW()")` ensures PostgreSQL sets timestamp at insert time, not SQLModel. This aligns with Database Schema Architect skill requirement for database-level automation. UPDATE trigger for `updated_at` must be created in migration (SQLModel doesn't support triggers directly).

**Alternatives Considered**:
- `default_factory=datetime.utcnow` - Application-level default, rejected for single source of truth violation
- Pydantic validator for auto-update - Doesn't work for direct SQL updates, rejected

### Index Definition Strategy

**Decision**: Define indexes in Alembic migrations, not SQLModel `__table_args__`

**Rationale**:
- Alembic autogeneration detects `Field(index=True)` but doesn't give control over index name
- Explicit migration allows naming convention: `idx_<table>_<column>`
- Easier to review and rollback index changes independently

```python
# In migration file (preferred)
op.create_index('idx_tasks_user_id', 'tasks', ['user_id'])

# In SQLModel (works, but less control)
class Task(SQLModel, table=True):
    __table_args__ = (
        Index('idx_tasks_user_id', 'user_id'),
    )
```

---

## 2. Alembic Migration Patterns

### Decision: Manual Trigger Creation in Initial Migration

**Rationale**: Alembic autogeneration doesn't detect PostgreSQL triggers. We must manually add trigger creation to the generated migration script.

### Initial Migration Structure

```python
"""Initial database schema

Revision ID: 001_initial_schema
Revises:
Create Date: 2026-01-12
"""

from alembic import op
import sqlalchemy as sa

def upgrade() -> None:
    # 1. Create tables (autogenerated by Alembic)
    op.create_table(
        'users',
        sa.Column('id', sa.Text(), nullable=False),
        # ... columns
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('email')
    )

    op.create_table('tasks', ...)

    # 2. Create indexes (explicit control)
    op.create_index('idx_tasks_user_id', 'tasks', ['user_id'])

    # 3. Create trigger function (MANUAL - not autogenerated)
    op.execute("""
        CREATE OR REPLACE FUNCTION update_updated_at_column()
        RETURNS TRIGGER AS $$
        BEGIN
            NEW.updated_at = NOW();
            RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;
    """)

    # 4. Create trigger (MANUAL - not autogenerated)
    op.execute("""
        CREATE TRIGGER update_tasks_updated_at
            BEFORE UPDATE ON tasks
            FOR EACH ROW
            EXECUTE FUNCTION update_updated_at_column();
    """)

def downgrade() -> None:
    # Reverse order: triggers → indexes → tables
    op.execute("DROP TRIGGER IF EXISTS update_tasks_updated_at ON tasks")
    op.execute("DROP FUNCTION IF EXISTS update_updated_at_column")
    op.drop_index('idx_tasks_user_id', 'tasks')
    op.drop_table('tasks')
    op.drop_table('users')
```

**Key Insight**: `op.execute()` for raw SQL (triggers, functions). Use `op.create_table()` for schema objects that Alembic can track.

### UPDATE Trigger Syntax

**PostgreSQL Trigger Pattern**:
1. Create function that modifies `NEW` record (BEFORE UPDATE trigger)
2. Create trigger that calls function on each row update
3. Trigger runs automatically before any UPDATE statement

**Testing**: Verify trigger works by updating a task and checking `updated_at` changed.

### Migration Testing Strategy

**Test Pattern** (upgrade → downgrade → upgrade):
```bash
# 1. Apply migration
alembic upgrade head

# 2. Verify schema exists
psql $DATABASE_URL -c "\dt"  # List tables
psql $DATABASE_URL -c "\d tasks"  # Describe tasks table

# 3. Rollback migration
alembic downgrade -1

# 4. Verify schema removed
psql $DATABASE_URL -c "\dt"  # Should show no tables

# 5. Re-apply migration
alembic upgrade head

# Result: If all steps succeed, migration is reversible
```

**Automated Testing** (pytest):
```python
def test_migration_upgrade_downgrade(test_db_engine):
    # Apply migration
    alembic_config = Config("alembic.ini")
    command.upgrade(alembic_config, "head")

    # Verify schema
    inspector = inspect(test_db_engine)
    assert "tasks" in inspector.get_table_names()
    assert "users" in inspector.get_table_names()

    # Rollback
    command.downgrade(alembic_config, "-1")

    # Verify removed
    inspector = inspect(test_db_engine)
    assert "tasks" not in inspector.get_table_names()

    # Re-apply
    command.upgrade(alembic_config, "head")
```

### Data Migration vs Schema Migration

**Decision**: Separate data migrations from schema migrations

**Pattern**:
- Schema migrations: Table structure, indexes, constraints (this feature)
- Data migrations: Insert/update/delete data (future feature, e.g., backfill user data)

**Rationale**: Mixing schema and data changes makes rollbacks risky. If downgrade drops table, data is lost. Keep separate for safety.

---

## 3. Neon PostgreSQL Connection Configuration

### Decision: Use Pooled Endpoint with Explicit Configuration

**Connection String Format**:
```
# Pooled endpoint (REQUIRED for serverless)
postgresql://user:password@ep-abc-123-pooler.us-east-2.aws.neon.tech:5432/dbname?sslmode=require

# Direct endpoint (DON'T USE in serverless)
postgresql://user:password@ep-abc-123.us-east-2.aws.neon.tech:5432/dbname
```

**Key Difference**: `-pooler` suffix routes through PgBouncer (connection pooling layer).

### SQLModel Engine Configuration

```python
# backend/src/database.py
import os
from sqlmodel import create_engine

DATABASE_URL = os.getenv("DATABASE_URL")

if not DATABASE_URL:
    raise ValueError("DATABASE_URL environment variable not set")

engine = create_engine(
    DATABASE_URL,
    echo=False,  # Disable SQL logging in production (enable for debugging)
    pool_pre_ping=True,  # Verify connection before use (detect stale connections)
    pool_size=1,  # Minimal pool for serverless (each instance has own pool)
    max_overflow=0,  # No overflow (PgBouncer handles pooling)
    connect_args={
        "connect_timeout": 10,  # Fail fast on connection issues (seconds)
        "options": "-c statement_timeout=30000"  # Query timeout 30s (milliseconds)
    }
)
```

**Rationale**:
- `pool_pre_ping=True`: Serverless functions go idle, connections become stale. Pre-ping detects and reconnects.
- `pool_size=1, max_overflow=0`: Minimal pool because PgBouncer handles pooling externally.
- `connect_timeout=10`: Fail fast if database unreachable (don't wait 60+ seconds).
- `statement_timeout=30000`: Kill runaway queries after 30 seconds (prevents resource exhaustion).

**Alternatives Considered**:
- `pool_size=20`: Rejected - wastes resources in serverless, PgBouncer already pools
- No `pool_pre_ping`: Rejected - causes intermittent "connection closed" errors after idle

### Environment Variable Management

**`.env` File Structure**:
```bash
# .env (NEVER commit to git, add to .gitignore)
DATABASE_URL=postgresql://user:password@ep-abc-123-pooler.us-east-2.aws.neon.tech:5432/todo_db?sslmode=require
BETTER_AUTH_SECRET=your-secret-key-minimum-32-characters
```

**Loading in Python**:
```python
# backend/src/config.py
from dotenv import load_dotenv
import os

load_dotenv()  # Loads .env file into os.environ

DATABASE_URL = os.getenv("DATABASE_URL")
BETTER_AUTH_SECRET = os.getenv("BETTER_AUTH_SECRET")
```

**Production**: Use cloud secrets manager (Vercel Environment Variables, AWS Secrets Manager), not `.env` file.

### Health Check Queries

**Decision**: Use `SELECT 1` for application health checks

```python
# backend/src/api/health.py
from fastapi import HTTPException, status
from sqlmodel import Session, text

@app.get("/health")
def health_check():
    try:
        with Session(engine) as session:
            session.exec(text("SELECT 1")).one()  # Simple query, fast
        return {"status": "healthy", "database": "connected"}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Database unavailable"
        )
```

**Alternatives Considered**:
- `pg_isready` command-line tool: Requires shell access, not suitable for API endpoint
- Query real tables: Slower, unnecessary for health check
- No health check: Rejected - prevents load balancer health monitoring

---

## 4. Testing Database Setup

### Decision: Use pytest-postgresql with Transactional Rollback

**Rationale**: Each test should run in isolation with a clean database state. Transactional rollback is faster than recreating database per test.

### pytest Fixture Configuration

```python
# backend/tests/conftest.py
import pytest
from sqlmodel import create_engine, Session
from alembic import command
from alembic.config import Config

@pytest.fixture(scope="session")
def test_database_url():
    """Provide test database URL (separate from production)."""
    # Option 1: In-memory SQLite (fast, no PostgreSQL features)
    # return "sqlite:///:memory:"

    # Option 2: Local PostgreSQL (requires running postgres)
    return "postgresql://postgres:postgres@localhost:5432/todo_test"

    # Option 3: pytest-postgresql plugin (auto-manages postgres instance)
    # Uses plugin's temporary database

@pytest.fixture(scope="session")
def test_engine(test_database_url):
    """Create test database engine and apply migrations."""
    engine = create_engine(test_database_url, echo=True)

    # Apply Alembic migrations to test database
    alembic_config = Config("alembic.ini")
    alembic_config.set_main_option("sqlalchemy.url", test_database_url)
    command.upgrade(alembic_config, "head")

    yield engine

    # Teardown: Drop all tables
    command.downgrade(alembic_config, "base")

@pytest.fixture
def session(test_engine):
    """Provide transactional session that rolls back after each test."""
    connection = test_engine.connect()
    transaction = connection.begin()
    session = Session(bind=connection)

    yield session

    session.close()
    transaction.rollback()  # Undo all changes from this test
    connection.close()
```

**Key Insight**: `scope="session"` for engine (reuse across tests), no scope for session (new per test with rollback).

### Transactional Test Isolation

**Pattern**:
1. Begin transaction before test
2. Run test (inserts/updates/deletes)
3. Rollback transaction after test (database state reset)

**Benefit**: 100x faster than recreating database per test. All tests see empty database.

### Test Database Initialization

**Migration Application**:
```python
# In conftest.py (shown above)
command.upgrade(alembic_config, "head")  # Apply all migrations
```

**Verification**:
```python
def test_schema_exists(test_engine):
    """Verify tables created by migrations."""
    from sqlalchemy import inspect
    inspector = inspect(test_engine)
    tables = inspector.get_table_names()

    assert "users" in tables
    assert "tasks" in tables

def test_indexes_exist(test_engine):
    """Verify indexes created by migrations."""
    from sqlalchemy import inspect
    inspector = inspect(test_engine)
    indexes = inspector.get_indexes("tasks")

    index_names = [idx["name"] for idx in indexes]
    assert "idx_tasks_user_id" in index_names
```

### Mocking vs Real Database

**Decision**: Use real PostgreSQL for integration tests, mocking only for unit tests

**Rationale**:
- **Integration Tests** (test database interactions): Real PostgreSQL required
  - Test migrations (upgrade/downgrade)
  - Test database constraints (foreign keys, NOT NULL)
  - Test triggers (updated_at automation)
  - Test query performance (<100ms requirement)

- **Unit Tests** (test business logic): Mocking acceptable
  - Test model validation (Pydantic)
  - Test service layer logic (no database)

**Example**:
```python
# Integration test (real database)
def test_cascade_deletion(session):
    """Verify ON DELETE CASCADE works."""
    user = User(id="user_1", email="test@example.com", name="Test")
    task = Task(user_id="user_1", title="Test Task")
    session.add(user)
    session.add(task)
    session.commit()

    # Delete user
    session.delete(user)
    session.commit()

    # Task should be cascade-deleted
    remaining_tasks = session.exec(select(Task)).all()
    assert len(remaining_tasks) == 0

# Unit test (mocked)
def test_task_validation():
    """Verify Pydantic validation (no database)."""
    with pytest.raises(ValueError):
        Task(title="x" * 201)  # Title max 200 chars
```

---

## Summary of Decisions

| Topic | Decision | Rationale |
|-------|----------|-----------|
| **BIGINT Primary Keys** | `sa_column=Column(BigInteger, autoincrement=True)` | Prevents ID exhaustion (~9.2 quintillion capacity) |
| **Foreign Keys** | `Field(foreign_key="users.id", nullable=False, index=True)` | Enforces referential integrity, creates index |
| **Timestamp Defaults** | `server_default=text("NOW()")` | Database-level automation, single source of truth |
| **Index Strategy** | Explicit creation in Alembic migration | Control over index names, easier rollback |
| **UPDATE Trigger** | Manual `op.execute()` in migration | Alembic doesn't autogenerate triggers |
| **Migration Testing** | Upgrade → downgrade → upgrade cycle | Verifies reversibility |
| **Neon Connection** | Pooled endpoint (`-pooler` suffix) | Required for serverless, prevents connection exhaustion |
| **Engine Config** | `pool_pre_ping=True, pool_size=1, connect_timeout=10` | Detects stale connections, minimal pool (PgBouncer pools externally) |
| **Environment Variables** | `.env` file (dev), secrets manager (prod) | Never commit secrets to git |
| **Health Check** | `SELECT 1` query | Fast, suitable for load balancer monitoring |
| **Test Database** | pytest-postgresql with transactional rollback | Fast test isolation, real PostgreSQL features |
| **Testing Strategy** | Real database for integration, mocking for unit | Integration tests require real PostgreSQL |

---

## References

- **SQLModel Documentation**: https://sqlmodel.tiangolo.com/
- **Alembic Documentation**: https://alembic.sqlalchemy.org/en/latest/
- **Neon PostgreSQL**: https://neon.tech/docs/
- **PgBouncer**: https://www.pgbouncer.org/
- **pytest-postgresql**: https://github.com/ClearcodeHQ/pytest-postgresql
- **Database Schema Architect Skill**: `.claude/skills/database_schema_architect.md`
- **Neon PostgreSQL Serverless Integration Skill**: `.claude/skills/neon_postgresql_serverless_integration.md`

---

**Status**: ✅ All research complete. Proceed to Phase 1 (data-model.md, contracts/, quickstart.md).
